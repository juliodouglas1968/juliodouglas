{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7MAIR Aprendizaje por Refuerzo - Ejercicio de evaluación1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliodouglas1968/juliodouglas/blob/master/7MAIR_Aprendizaje_por_Refuerzo_Ejercicio_de_evaluaci%C3%B3n1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4D4u3GuzOYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0eba37f7-fc6b-48b1-92bc-2912a5937227"
      },
      "source": [
        "!pip install gym\n",
        "\n",
        "!pip install h5py\n",
        "\n",
        "!pip install Pillow\n",
        "\n",
        "!pip install gym[atari]\n",
        "\n",
        "!pip install keras-rl\n",
        "\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install keras-rl==0.4.2\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (3.4.7.28)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.4.7.28)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.4)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n",
            "Collecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.17.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.3.2)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=1293e7a530a40754497d8f927e310b6df1400afeb68f0f10c17926634feaec61\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 35kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.6.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
            "Requirement already satisfied: keras-rl==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl==0.4.2) (2.2.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.17.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTKo9bn7z5iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c29a6662-eb9e-4e0c-839a-773f23720f92"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpxXj4nK0JXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'PongDeterministic-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZMn1fgh3yFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rl.core import Processor\n",
        "# Define the input shape to resize the screen\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "# This processor will be similar to the Atari processor\n",
        "class PongProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        \n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        \n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iId9vwxi0Xm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "deb6123d-17de-4caf-863e-28ef2eef0baf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 100800)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                1612816   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 102       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 1,613,462\n",
            "Trainable params: 1,613,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dD-1Uug05Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's define the memory for storing the experience\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "# Define the policy that our agent will follow\n",
        "policy = BoltzmannQPolicy()\n",
        "\n",
        "\n",
        "# Define the agent\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "optimizer = Adam(lr=1e-3)\n",
        "dqn.compile(optimizer, metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz2UwNeR1Q79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dee076a-a135-493d-a735-6455fecbc4c6"
      },
      "source": [
        "# Train the agent\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "\n",
        "# After training is done, we save the final weights.\n",
        "#dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   764/50000: episode: 1, duration: 47.968s, episode steps: 764, steps per second: 16, episode reward: -21.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.616 [0.000, 5.000], mean observation: 98.094 [0.000, 236.000], loss: 2741.843194, mean_absolute_error: 1.774980, mean_q: 3.304803\n",
            "  1651/50000: episode: 2, duration: 53.377s, episode steps: 887, steps per second: 17, episode reward: -20.000, mean reward: -0.023 [-1.000, 1.000], mean action: 2.454 [0.000, 5.000], mean observation: 98.037 [0.000, 236.000], loss: 2184.275146, mean_absolute_error: 1.888542, mean_q: 5.150588\n",
            "  2661/50000: episode: 3, duration: 61.778s, episode steps: 1010, steps per second: 16, episode reward: -19.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.528 [0.000, 5.000], mean observation: 97.989 [0.000, 236.000], loss: 0.012082, mean_absolute_error: 0.060135, mean_q: -0.051341\n",
            "  3485/50000: episode: 4, duration: 51.156s, episode steps: 824, steps per second: 16, episode reward: -21.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.540 [0.000, 5.000], mean observation: 98.102 [0.000, 236.000], loss: 0.012905, mean_absolute_error: 0.096964, mean_q: -0.093934\n",
            "  4601/50000: episode: 5, duration: 71.205s, episode steps: 1116, steps per second: 16, episode reward: -19.000, mean reward: -0.017 [-1.000, 1.000], mean action: 2.529 [0.000, 5.000], mean observation: 97.971 [0.000, 236.000], loss: 0.011256, mean_absolute_error: 0.167596, mean_q: -0.183396\n",
            "  5425/50000: episode: 6, duration: 52.087s, episode steps: 824, steps per second: 16, episode reward: -21.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.465 [0.000, 5.000], mean observation: 98.073 [0.000, 236.000], loss: 0.011860, mean_absolute_error: 0.270749, mean_q: -0.305988\n",
            "  6372/50000: episode: 7, duration: 59.460s, episode steps: 947, steps per second: 16, episode reward: -20.000, mean reward: -0.021 [-1.000, 1.000], mean action: 2.502 [0.000, 5.000], mean observation: 97.948 [0.000, 236.000], loss: 0.011299, mean_absolute_error: 0.363724, mean_q: -0.416728\n",
            "  7270/50000: episode: 8, duration: 56.054s, episode steps: 898, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.557 [0.000, 5.000], mean observation: 97.975 [0.000, 236.000], loss: 0.011536, mean_absolute_error: 0.457399, mean_q: -0.529350\n",
            "  8294/50000: episode: 9, duration: 65.149s, episode steps: 1024, steps per second: 16, episode reward: -21.000, mean reward: -0.021 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 98.096 [0.000, 236.000], loss: 0.011245, mean_absolute_error: 0.550269, mean_q: -0.638959\n",
            "  9295/50000: episode: 10, duration: 62.357s, episode steps: 1001, steps per second: 16, episode reward: -19.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.542 [0.000, 5.000], mean observation: 98.012 [0.000, 236.000], loss: 0.010832, mean_absolute_error: 0.631542, mean_q: -0.735842\n",
            " 10416/50000: episode: 11, duration: 71.443s, episode steps: 1121, steps per second: 16, episode reward: -19.000, mean reward: -0.017 [-1.000, 1.000], mean action: 2.486 [0.000, 5.000], mean observation: 98.012 [0.000, 236.000], loss: 0.009920, mean_absolute_error: 0.696420, mean_q: -0.815589\n",
            " 11344/50000: episode: 12, duration: 56.840s, episode steps: 928, steps per second: 16, episode reward: -21.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.557 [0.000, 5.000], mean observation: 98.079 [0.000, 236.000], loss: 0.011000, mean_absolute_error: 0.768726, mean_q: -0.901638\n",
            " 12442/50000: episode: 13, duration: 68.383s, episode steps: 1098, steps per second: 16, episode reward: -20.000, mean reward: -0.018 [-1.000, 1.000], mean action: 2.496 [0.000, 5.000], mean observation: 97.937 [0.000, 236.000], loss: 0.011136, mean_absolute_error: 0.831154, mean_q: -0.973970\n",
            " 13206/50000: episode: 14, duration: 47.715s, episode steps: 764, steps per second: 16, episode reward: -21.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.551 [0.000, 5.000], mean observation: 98.087 [0.000, 236.000], loss: 0.010050, mean_absolute_error: 0.880904, mean_q: -1.035467\n",
            " 14284/50000: episode: 15, duration: 70.753s, episode steps: 1078, steps per second: 15, episode reward: -19.000, mean reward: -0.018 [-1.000, 1.000], mean action: 2.482 [0.000, 5.000], mean observation: 97.968 [0.000, 236.000], loss: 0.011270, mean_absolute_error: 0.930889, mean_q: -1.093301\n",
            " 15289/50000: episode: 16, duration: 66.213s, episode steps: 1005, steps per second: 15, episode reward: -19.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.552 [0.000, 5.000], mean observation: 97.991 [0.000, 236.000], loss: 0.010906, mean_absolute_error: 0.987852, mean_q: -1.163434\n",
            " 16175/50000: episode: 17, duration: 56.427s, episode steps: 886, steps per second: 16, episode reward: -21.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.414 [0.000, 5.000], mean observation: 98.075 [0.000, 236.000], loss: 0.010932, mean_absolute_error: 1.035105, mean_q: -1.217110\n",
            " 17106/50000: episode: 18, duration: 56.846s, episode steps: 931, steps per second: 16, episode reward: -20.000, mean reward: -0.021 [-1.000, 1.000], mean action: 2.531 [0.000, 5.000], mean observation: 98.024 [0.000, 236.000], loss: 0.011413, mean_absolute_error: 1.066507, mean_q: -1.255169\n",
            " 18022/50000: episode: 19, duration: 55.394s, episode steps: 916, steps per second: 17, episode reward: -21.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.522 [0.000, 5.000], mean observation: 98.074 [0.000, 236.000], loss: 0.011277, mean_absolute_error: 1.103339, mean_q: -1.298069\n",
            " 18972/50000: episode: 20, duration: 57.708s, episode steps: 950, steps per second: 16, episode reward: -20.000, mean reward: -0.021 [-1.000, 1.000], mean action: 2.462 [0.000, 5.000], mean observation: 98.038 [0.000, 236.000], loss: 0.011215, mean_absolute_error: 1.133513, mean_q: -1.335354\n",
            " 19904/50000: episode: 21, duration: 56.442s, episode steps: 932, steps per second: 17, episode reward: -21.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.466 [0.000, 5.000], mean observation: 98.056 [0.000, 236.000], loss: 0.010962, mean_absolute_error: 1.159383, mean_q: -1.366413\n",
            " 20806/50000: episode: 22, duration: 54.204s, episode steps: 902, steps per second: 17, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.496 [0.000, 5.000], mean observation: 97.904 [0.000, 236.000], loss: 0.011617, mean_absolute_error: 1.185793, mean_q: -1.397678\n",
            " 21861/50000: episode: 23, duration: 64.149s, episode steps: 1055, steps per second: 16, episode reward: -19.000, mean reward: -0.018 [-1.000, 1.000], mean action: 2.499 [0.000, 5.000], mean observation: 97.990 [0.000, 236.000], loss: 0.011294, mean_absolute_error: 1.203600, mean_q: -1.417137\n",
            " 22819/50000: episode: 24, duration: 58.228s, episode steps: 958, steps per second: 16, episode reward: -20.000, mean reward: -0.021 [-1.000, 1.000], mean action: 2.496 [0.000, 5.000], mean observation: 97.929 [0.000, 236.000], loss: 0.010134, mean_absolute_error: 1.215297, mean_q: -1.434446\n",
            " 23845/50000: episode: 25, duration: 62.059s, episode steps: 1026, steps per second: 17, episode reward: -19.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.549 [0.000, 5.000], mean observation: 98.042 [0.000, 236.000], loss: 0.011044, mean_absolute_error: 1.234476, mean_q: -1.455832\n",
            " 24715/50000: episode: 26, duration: 52.613s, episode steps: 870, steps per second: 17, episode reward: -21.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.466 [0.000, 5.000], mean observation: 98.084 [0.000, 236.000], loss: 0.010750, mean_absolute_error: 1.255276, mean_q: -1.482129\n",
            " 25867/50000: episode: 27, duration: 70.770s, episode steps: 1152, steps per second: 16, episode reward: -19.000, mean reward: -0.016 [-1.000, 1.000], mean action: 2.477 [0.000, 5.000], mean observation: 98.014 [0.000, 236.000], loss: 0.011341, mean_absolute_error: 1.271273, mean_q: -1.497981\n",
            " 26719/50000: episode: 28, duration: 50.718s, episode steps: 852, steps per second: 17, episode reward: -21.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.522 [0.000, 5.000], mean observation: 98.088 [0.000, 236.000], loss: 0.010995, mean_absolute_error: 1.284557, mean_q: -1.515506\n",
            " 27670/50000: episode: 29, duration: 58.026s, episode steps: 951, steps per second: 16, episode reward: -20.000, mean reward: -0.021 [-1.000, 1.000], mean action: 2.447 [0.000, 5.000], mean observation: 98.020 [0.000, 236.000], loss: 0.011175, mean_absolute_error: 1.299754, mean_q: -1.534234\n",
            " 29105/50000: episode: 30, duration: 87.898s, episode steps: 1435, steps per second: 16, episode reward: -16.000, mean reward: -0.011 [-1.000, 1.000], mean action: 2.466 [0.000, 5.000], mean observation: 98.064 [0.000, 236.000], loss: 0.011134, mean_absolute_error: 1.319969, mean_q: -1.557231\n",
            " 29931/50000: episode: 31, duration: 50.409s, episode steps: 826, steps per second: 16, episode reward: -21.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.459 [0.000, 5.000], mean observation: 98.073 [0.000, 236.000], loss: 0.010507, mean_absolute_error: 1.325541, mean_q: -1.565869\n",
            " 30829/50000: episode: 32, duration: 55.063s, episode steps: 898, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.608 [0.000, 5.000], mean observation: 97.960 [0.000, 236.000], loss: 0.011249, mean_absolute_error: 1.331207, mean_q: -1.570661\n",
            " 31709/50000: episode: 33, duration: 53.290s, episode steps: 880, steps per second: 17, episode reward: -21.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.591 [0.000, 5.000], mean observation: 98.093 [0.000, 236.000], loss: 0.011430, mean_absolute_error: 1.342577, mean_q: -1.583587\n",
            " 32602/50000: episode: 34, duration: 54.121s, episode steps: 893, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.548 [0.000, 5.000], mean observation: 98.055 [0.000, 236.000], loss: 0.010777, mean_absolute_error: 1.338586, mean_q: -1.579830\n",
            " 33523/50000: episode: 35, duration: 55.943s, episode steps: 921, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.503 [0.000, 5.000], mean observation: 97.999 [0.000, 236.000], loss: 0.010847, mean_absolute_error: 1.340472, mean_q: -1.582372\n",
            " 34423/50000: episode: 36, duration: 54.549s, episode steps: 900, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.416 [0.000, 5.000], mean observation: 98.009 [0.000, 236.000], loss: 0.011219, mean_absolute_error: 1.349993, mean_q: -1.593779\n",
            " 35364/50000: episode: 37, duration: 57.924s, episode steps: 941, steps per second: 16, episode reward: -21.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.533 [0.000, 5.000], mean observation: 98.069 [0.000, 236.000], loss: 0.011136, mean_absolute_error: 1.357500, mean_q: -1.602857\n",
            " 36206/50000: episode: 38, duration: 51.096s, episode steps: 842, steps per second: 16, episode reward: -20.000, mean reward: -0.024 [-1.000, 1.000], mean action: 2.551 [0.000, 5.000], mean observation: 98.051 [0.000, 236.000], loss: 0.011082, mean_absolute_error: 1.363579, mean_q: -1.610644\n",
            " 37090/50000: episode: 39, duration: 53.860s, episode steps: 884, steps per second: 16, episode reward: -21.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.510 [0.000, 5.000], mean observation: 98.098 [0.000, 236.000], loss: 0.011169, mean_absolute_error: 1.373034, mean_q: -1.620261\n",
            " 38011/50000: episode: 40, duration: 55.790s, episode steps: 921, steps per second: 17, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.532 [0.000, 5.000], mean observation: 97.989 [0.000, 236.000], loss: 0.011097, mean_absolute_error: 1.373929, mean_q: -1.622317\n",
            " 38848/50000: episode: 41, duration: 50.664s, episode steps: 837, steps per second: 17, episode reward: -20.000, mean reward: -0.024 [-1.000, 1.000], mean action: 2.404 [0.000, 5.000], mean observation: 98.068 [0.000, 236.000], loss: 0.011947, mean_absolute_error: 1.371859, mean_q: -1.619063\n",
            " 39899/50000: episode: 42, duration: 64.523s, episode steps: 1051, steps per second: 16, episode reward: -20.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.455 [0.000, 5.000], mean observation: 98.124 [0.000, 236.000], loss: 0.010686, mean_absolute_error: 1.377095, mean_q: -1.627574\n",
            " 40801/50000: episode: 43, duration: 54.888s, episode steps: 902, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.621 [0.000, 5.000], mean observation: 97.966 [0.000, 236.000], loss: 0.011095, mean_absolute_error: 1.388948, mean_q: -1.640138\n",
            " 41772/50000: episode: 44, duration: 58.557s, episode steps: 971, steps per second: 17, episode reward: -21.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.568 [0.000, 5.000], mean observation: 98.063 [0.000, 236.000], loss: 0.010777, mean_absolute_error: 1.393115, mean_q: -1.646060\n",
            " 42782/50000: episode: 45, duration: 60.895s, episode steps: 1010, steps per second: 17, episode reward: -19.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.644 [0.000, 5.000], mean observation: 98.045 [0.000, 236.000], loss: 0.011133, mean_absolute_error: 1.395822, mean_q: -1.648043\n",
            " 43666/50000: episode: 46, duration: 53.394s, episode steps: 884, steps per second: 17, episode reward: -21.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.464 [0.000, 5.000], mean observation: 98.085 [0.000, 236.000], loss: 0.010403, mean_absolute_error: 1.396536, mean_q: -1.652162\n",
            " 44458/50000: episode: 47, duration: 48.443s, episode steps: 792, steps per second: 16, episode reward: -21.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.677 [0.000, 5.000], mean observation: 98.083 [0.000, 236.000], loss: 0.012228, mean_absolute_error: 1.405599, mean_q: -1.659607\n",
            " 45241/50000: episode: 48, duration: 49.049s, episode steps: 783, steps per second: 16, episode reward: -21.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.508 [0.000, 5.000], mean observation: 98.060 [0.000, 236.000], loss: 0.011550, mean_absolute_error: 1.419190, mean_q: -1.677252\n",
            " 46228/50000: episode: 49, duration: 58.578s, episode steps: 987, steps per second: 17, episode reward: -20.000, mean reward: -0.020 [-1.000, 1.000], mean action: 2.476 [0.000, 5.000], mean observation: 98.033 [0.000, 236.000], loss: 0.011041, mean_absolute_error: 1.423225, mean_q: -1.680622\n",
            " 47152/50000: episode: 50, duration: 56.007s, episode steps: 924, steps per second: 16, episode reward: -20.000, mean reward: -0.022 [-1.000, 1.000], mean action: 2.515 [0.000, 5.000], mean observation: 98.094 [0.000, 236.000], loss: 0.011078, mean_absolute_error: 1.423503, mean_q: -1.683335\n",
            " 48023/50000: episode: 51, duration: 52.394s, episode steps: 871, steps per second: 17, episode reward: -21.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.476 [0.000, 5.000], mean observation: 98.093 [0.000, 236.000], loss: 0.011221, mean_absolute_error: 1.434689, mean_q: -1.696998\n",
            " 48815/50000: episode: 52, duration: 47.651s, episode steps: 792, steps per second: 17, episode reward: -21.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.376 [0.000, 5.000], mean observation: 98.084 [0.000, 236.000], loss: 0.011085, mean_absolute_error: 1.441989, mean_q: -1.705783\n",
            " 49814/50000: episode: 53, duration: 60.215s, episode steps: 999, steps per second: 17, episode reward: -19.000, mean reward: -0.019 [-1.000, 1.000], mean action: 2.517 [0.000, 5.000], mean observation: 98.071 [0.000, 236.000], loss: 0.011349, mean_absolute_error: 1.442762, mean_q: -1.704787\n",
            "done, took 3090.451 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc1c767b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdurgmhlb1ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After training is done, we save the final weights.\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}